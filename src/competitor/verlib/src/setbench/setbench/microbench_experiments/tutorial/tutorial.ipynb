{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data framework: the basic paradigm\n",
    "\n",
    "user implements one function `define_experiment`\n",
    "\n",
    "then runs `../../tools/data_framework/run_experiment.py`\n",
    "\n",
    "it runs potentially many experimental trials (over all defined configurations), captures output, builds a sqlite database, queries it, produces plots, and produces html pages to display plots...\n",
    "\n",
    "the data framework also provides lots of tools to do querying, plot generation and analysis in jupyter notebooks (see `instructions_data.ipynb`).\n",
    "\n",
    "none of this is specific to setbench! easy to apply to other code bases, as well. (data_framework is self contained--no dependencies on setbench.)\n",
    "\n",
    "### The following tutorial fully explains the derivation of several non-trivial `define_experiment()` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the following code cell before any others\n",
    "\n",
    "It does basic initialization for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "print(\"Initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 'hello world' of `run_experiment.sh`\n",
    "\n",
    "defining a trivial experiment that compiles and runs a single command once and saves the output.\n",
    "\n",
    "we do `run_in_jupyter` and pass `define_experiment`. could alternatively save `define_experiment` in a python file and run the equivalent `run_experiments.sh` command (described in comments)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from _basic_functions import *\n",
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')     ## working dir for compiling\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin') ## working dir for running\n",
    "    set_cmd_compile  (exp_dict, 'make brown_ext_abtree_lf.debra')\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./brown_ext_abtree_lf.debra -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 1000')\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-cr')\n",
    "# if the define_experiment() function above were saved in a file myexp.py,\n",
    "# then the run_in_jupyter line above is equivalent to running shell command:\n",
    "#   ../../tools/data_framework/run_experiment.py myexp.py -cr\n",
    "#\n",
    "# NOTE: -c causes COMPILATION to occur, and -r causes experiments to be RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the same thing from the command line!\n",
    "\n",
    "- create a file called `myexp.py` in this directory.\n",
    "- start it with `from _basic_functions import *`\n",
    "- copy the `define_experiment` function above into `myexp.py`\n",
    "- run `../../tools/data_framework/run_experiment.py myexp.py -cr` in the shell (starting from this directory)\n",
    "\n",
    "if you get an error along the lines of:\n",
    "\n",
    "`NameError: name 'set_dir_compile' is not defined`\n",
    "\n",
    "then you probably forgot to start the file with `from _basic_functions import *`, which is needed in any file where you define a `define_experiment` function for use with `run_experiment.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Re)running results without compiling\n",
    "\n",
    "you can rerun experiments without compiling by omitting `-c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')     ## working dir for compiling\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin') ## working dir for running\n",
    "    set_cmd_compile  (exp_dict, 'make brown_ext_abtree_lf.debra')\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./brown_ext_abtree_lf.debra -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 1000')\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-r')\n",
    "# equiv cmd: [...]/run_experiment.py myexp.py -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data files (captured stdout/err)\n",
    "\n",
    "every time the data_framework runs your \"run command\" (provided by `set_cmd_run`), the output is automatically saved in a `data file`.\n",
    "\n",
    "this is the output of that one run we executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(shell_to_str('cat data/data000001.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running with varying `run param`eters\n",
    "\n",
    "of course running one command isn't very interesting... you could do that yourself.\n",
    "\n",
    "instead, we want to run the command many times, with different arguments. to this end, we allow the user to specify `run param`s.\n",
    "\n",
    "the idea is as follows:\n",
    "- call `add_run_param` to make the data framework aware of parameters that you want your experiments to be run with.\n",
    "- your program will be run once for each set of values in the CROSS PRODUCT of all parameters.\n",
    "- (i.e., we will run your program with every combination of parameters)\n",
    "\n",
    "### Replacement strings / tokens\n",
    "\n",
    "you can use any of the run params you define to dynamically replace `{_tokens_like_this}` in the run command. for example, we include `{DS_TYPENAME}` in our run command, and it will be replaced by the current value of `{DS_TYPENAME}`. (that's right, we can run different commands based on the current value of `DS_TYPENAME`.)\n",
    "    \n",
    "you can also get the paths to key directories by using:\n",
    "- `{__dir_compile}`\n",
    "- `{__dir_run}`\n",
    "- `{__dir_data}`\n",
    "\n",
    "the following replacement token is also defined for you:\n",
    "- `{__step}`            the number of runs done so far, padded to six digits with leading zeros\n",
    "\n",
    "*note:* we now need to compile ALL of the binaries we want to *run*. so, we just change our make command to compile everything...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6') ## -j specifies how many threads to compile with\n",
    "\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 1000')\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-cr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data fields from captured stdout/err\n",
    "\n",
    "NOW we're going to EXTRACT data automatically from the generated data file(s). To do this, we must include the argument `-d` which stands for `database creation`.\n",
    "\n",
    "note 3 data files were produced this time: one for each value of `DS_TYPENAME`. let's put those data files to use by specifying that we want to *extract* some text from each data file.\n",
    "\n",
    "in particular, let's extract a line of the form \"`DS_TYPENAME=...`\" and a line of the form \"`total_throughput=...`\" from each data file. (you can find such lines in the data file above if you like.)\n",
    "\n",
    "extracted data is stored in a sqlite database `data/output_database.sqlite` in a table called `data`. (each field name passed to `add_data_field` becomes a **column** in `data`.)\n",
    "\n",
    "to specify a column to be extracted, we call `add_data_field()`. we do this for `total_throughput`, but note that we do *not* have to do this for `DS_TYPENAME`, as it was already added as a `run param`.\n",
    "\n",
    "whenever you add a data field, you should choose a column type `coltype` from:\n",
    "- `'TEXT'`\n",
    "- `'INTEGER'`\n",
    "- `'REAL'`\n",
    "\n",
    "the `default` if you do not specify is `'TEXT'`. note, however, that allowing the default `'TEXT'` option for a `numeric` field can cause problems when it is time to produce **graphs/plots**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER')\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying the database\n",
    "\n",
    "Note that we can simply **access** the last database we created, *WITHOUT rerunning* any experiments, by omitting all command line args in our `run_in_jupyter` call.\n",
    "\n",
    "Also note that you can accomplish the same thing from the **command line** by running `../../tools/data_framework/run_experiment.py myexp.py` with `cmdline_args` omitted. However, since you can't pass your `define_experiments` function as a command line argument, you have to save it in a `.py` file and pass the name `myexp.py` of that file as the first argument to `run_experiment.py`.\n",
    "\n",
    "To query the database, we can use function `select_to_dataframe(sql_string)` with a suitable `sql_string`. There are many other powerful functions included for querying and plotting data, but those are covered in `microbench_experiments/example/instructions_data.ipynb`. In **this** notebook we are focusing on the design of the `define_experiment` function.\n",
    "\n",
    "## Extra columns\n",
    "\n",
    "Note that the resulting query shows numerous extra columns such as `__hostname`, `__step` and `__cmd_run`, that we did *not* add ourselves. These are added *automatically* by the data framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='')\n",
    "df = select_to_dataframe('select * from data')\n",
    "df\n",
    "\n",
    "# run_in_jupyter call above has equivalent command:\n",
    "# [...]/run_experiment.py myexp.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppressing logging output in `run_in_jupyter`\n",
    "\n",
    "If you want to call `run_in_jupyter` as above *without* seeing the `logging data` that was copied to stdout, you can disable the log output by calling `disable_tee_stdout()`. Note that logs will still be collected, but the output will **only** go to the log file `output_log.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "disable_tee_stdout()\n",
    "run_in_jupyter(define_experiment, cmdline_args='')\n",
    "df = select_to_dataframe('select * from data')\n",
    "enable_tee_stdout() ## remember to enable, or you won't get output where you DO expect it...\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running multiple trials\n",
    "\n",
    "if you want to perform repeated trials of each experimental configuration, add a run_param called \"`__trials`\", and specify a list of trial numbers (as below).\n",
    "\n",
    "(the run_param doesn't *need* to be called `__trials` exactly, but if it is called `__trials` exactly,\n",
    "then extra sanity checks will be performed to verify, for example, that each data point in a graphical plot\n",
    "represents the average of precisely as many experimental runs as there are entries in the `__trials` list.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2, 3])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER')\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the data (to see the multiple trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_to_dataframe('select * from data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractors: mining data from arbitrary text\n",
    "\n",
    "by default, when you call `add_data_field(exp_dict, 'XYZ')`, a field `'XYZ'` will be fetched from each data file using extractor `grep_line()`, which greps (searches) for a line of the form `'XYZ={arbitrary string}\\n'`\n",
    "\n",
    "*if a field you want to extract is not stored that way in the output data*, then you can specify a custom `extractor` function, as we do in our example with `get_maxres()` below, to extract the max resident size from the 6th space-separated column of the output of the linux \"time\" command.\n",
    "\n",
    "also note: each field added with `add_data_field` becomes a replacement token (e.g., `{DS_TYPENAME}`) that can be references in any plot titles, axis titles, field lists, etc. (which we will see more on below).\n",
    "\n",
    "the following special fields are also defined for you (and added to the `data` table):\n",
    "- `{__step}`            the number of runs done so far, padded to six digits with leading zeros\n",
    "- `{__cmd_run}`         your cmd_run string with any tokens replaced appropriately for this run\n",
    "- `{__file_data}`       the output filename for the current run's data\n",
    "- `{__path_data}`       the relative path to the output file for the current run's data\n",
    "- `{__hostname}`        the result of running the hostname command on the machine\n",
    "- `{__id}`              a unique row ID\n",
    "\n",
    "note: in the following, `defaults` are `validator=is_nonempty` and `extractor=grep_line`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text output we are *trying* to extract max resident size from\n",
    "\n",
    "A line of the form:\n",
    "\n",
    "`960.43user 50.70system 0:06.14elapsed 16449%CPU (0avgtext+0avgdata 3034764maxresident)k`\n",
    "\n",
    "From this, we would like to extract `3034764`, then convert from KB to MB..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractor that accomplishes this\n",
    "\n",
    "`input`: an `extractor` function takes, as its arguments: the same `exp_dict` argument as `define_experiment()`, a `file_name` to load data from, and a `field_name` to extract.\n",
    "\n",
    "`processing`: it should fetch the appropriate contents for that field, from the given `file_name` and return them.\n",
    "\n",
    "`output`: return type can be a `string`, `int` or `float`.\n",
    "\n",
    "(in cases like this, where we're writing a custom `extractor` to fetch a specific field, the `field_name` argument ends up being irrelevant.)\n",
    "\n",
    "you are free to read the contents of the file, and process the data you see however you like, to come up with the desired return value.\n",
    "\n",
    "in our case, we will use the `shell_to_str()` utility function provided by the data framework to run a sequence of `bash` shell commands to extract the desired string from the file, then cast it to a `float` and convert it from kilobytes to megabytes.\n",
    "\n",
    "## (you could just as easily do this with pure python code. the choice is yours.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maxres(exp_dict, file_name, field_name):\n",
    "    ## manually parse the maximum resident size from the output of `time` and add it to the data file\n",
    "    maxres_kb_str = shell_to_str('grep \"maxres\" {} | cut -d\" \" -f6 | cut -d\"m\" -f1'.format(file_name))\n",
    "    return float(maxres_kb_str) / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using** this extractor in `define_experiment`\n",
    "\n",
    "we actually use this extractor by adding a data field and specifying it:\n",
    "\n",
    "`add_data_field   (exp_dict, 'maxresident_mb', extractor=get_maxres)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_maxres(exp_dict, file_name, field_name):\n",
    "    ## manually parse the maximum resident size from the output of `time` and add it to the data file\n",
    "    maxres_kb_str = shell_to_str('grep \"maxres\" {} | cut -d\" \" -f6 | cut -d\"m\" -f1'.format(file_name))\n",
    "    return float(maxres_kb_str) / 1000\n",
    "\n",
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2, 3])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'maxresident_mb', coltype='REAL', extractor=get_maxres)\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the resulting data\n",
    "\n",
    "note the `maxresident_mb` column -- highlighted for emphasis using Pandas DataFrame `style.applymap()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = select_to_dataframe('select * from data')\n",
    "\n",
    "df.style.applymap(lambda s: 'background-color: #b63f3f', subset=pd.IndexSlice[:, ['maxresident_mb']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validators: *checking* extracted data\n",
    "\n",
    "suppose you want to run some basic *sanity checks* on fields you pull from data files.\n",
    "\n",
    "a `validator` function is a great way of having the data framework perform a basic check on values as they are extracted from data files.\n",
    "\n",
    "pre-existing `validator` functions:\n",
    "- `is_positive`\n",
    "- `is_nonempty`\n",
    "- `is_equal(to_value)`\n",
    "\n",
    "for example, suppose we want to verify that `total_throughput` and `maxresident_mb` are both **positive** numbers. to do this, we specify `validator=is_positive` for each, below.\n",
    "\n",
    "note: you can write your own `validator` by mimicking the ones in `../../tools/data_framework/_basic_functions.py`. (see `is_positive` and `is_equal`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_maxres(exp_dict, file_name, field_name):\n",
    "    ## manually parse the maximum resident size from the output of `time` and add it to the data file\n",
    "    maxres_kb_str = shell_to_str('grep \"maxres\" {} | cut -d\" \" -f6 | cut -d\"m\" -f1'.format(file_name))\n",
    "    return float(maxres_kb_str) / 1000\n",
    "\n",
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2, 3])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "    add_data_field   (exp_dict, 'maxresident_mb', coltype='REAL', extractor=get_maxres, validator=is_positive)\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens when a field *fails* validation?\n",
    "\n",
    "we trigger a validation failure by specifying an obviously incorrect validator `is_equal('hello')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_maxres(exp_dict, file_name, field_name):\n",
    "    ## manually parse the maximum resident size from the output of `time` and add it to the data file\n",
    "    maxres_kb_str = shell_to_str('grep \"maxres\" {} | cut -d\" \" -f6 | cut -d\"m\" -f1'.format(file_name))\n",
    "    return float(maxres_kb_str) / 1000\n",
    "\n",
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2, 3])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_equal('hello'))\n",
    "    add_data_field   (exp_dict, 'maxresident_mb', coltype='REAL', extractor=get_maxres, validator=is_positive)\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rd', error_exit_code=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results (for data with <ins>3 dimensions</ins>)\n",
    "\n",
    "One of the main reasons I created the data framework was to make it stupid-easy to produce lots of graphs/plots.\n",
    "\n",
    "The main tool for doing this is the `add_plot_set` function.\n",
    "\n",
    "`add_plot_set()` can be used to cause a SET of plots to be rendered as images in the data directory.\n",
    "\n",
    "the precise SET of plots is defined by the fields included in `varying_cols_list` keyword argument.\n",
    " (the data framework will iterate over all distinct combinations of values in `varying_cols_list`,\n",
    " and will render a plot for each.)\n",
    " in the example below, we do *not* pass any `varying_cols_list` argument, so only a single plot is produced.\n",
    "\n",
    "(we will see where `varying_cols_list` is useful, and how it is used, in some of the later examples...)\n",
    "\n",
    "Note: a plot's title and filename can only use replacement `{tokens}` that correspond\n",
    "      to fields THAT ARE INCLUDED in `varying_cols_list[]`.\n",
    "      (this is because only those tokens are well defined and unique PER PLOT)\n",
    "\n",
    "### Note: any plots you define are *not actually rendered* unless you add command line argument `-p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools') ## tools library for plotting\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 2, 4, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel 5 5 -k 200000 -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "\n",
    "    add_plot_set(\n",
    "            exp_dict\n",
    "          , name='throughput.png'\n",
    "          , title='Throughput vs data structure'\n",
    "          , series='DS_TYPENAME'\n",
    "          , x_axis='TOTAL_THREADS'\n",
    "          , y_axis='total_throughput'\n",
    "          , plot_type='bars', plot_cmd_args = '--legend-include'\n",
    "    )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rdp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's view the data and plot produced by the previous cell\n",
    "\n",
    "(You have to run the previous cell before running the next one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "display(Image('data/throughput.png'))\n",
    "display(select_to_dataframe('select * from data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting data with a custom function\n",
    "\n",
    "If you want full control over how your data is plotted, you can specify your own function as the `plot_type` argument.\n",
    "\n",
    "Your custom function will be called with keyword arguments:\n",
    "- `filename`        -- the output filename for the plot image\n",
    "- `column_filters`  -- the *current* values of all fields in `varying_cols_list` (if any)\n",
    "- `data`            -- a Pandas DataFrame containing the (filtered) data for this plot\n",
    "- `series_name`     -- name of the column containing `series` in `data` (`''` if no series)\n",
    "- `x_name`          -- name of the column containing `x-values` in `data`\n",
    "- `y_name`          -- name of the column containing `y-values` in `data`\n",
    "- `exp_dict`        -- same as `exp_dict` passed to `define_experiment`\n",
    "\n",
    "To *better understand* what data is passed to a custom function, let's create a custom function that just prints its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_plot_func(filename, column_filters, data, series_name, x_name, y_name, exp_dict=None):\n",
    "    print('## filename: {}'.format(filename))\n",
    "    print('## filters: {}'.format(column_filters))\n",
    "    print('## data:')\n",
    "    print(data)\n",
    "\n",
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools')\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 2, 4, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel 5 5 -k 200000 -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "\n",
    "    add_plot_set(\n",
    "            exp_dict\n",
    "          , name='throughput.png'\n",
    "          , title='Throughput vs data structure'\n",
    "          , series='DS_TYPENAME'\n",
    "          , x_axis='TOTAL_THREADS'\n",
    "          , y_axis='total_throughput'\n",
    "          , plot_type=my_plot_func\n",
    "    )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "\n",
    "disable_tee_stdout() ## disable regular log printing so we ONLY see OUR prints below\n",
    "run_in_jupyter(define_experiment, cmdline_args='-dp')\n",
    "enable_tee_stdout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For example, we can plot this data *manually* using `Pandas`\n",
    "\n",
    "Since we have `TWO trials` per combination of `DS_TYPENAME` and `TOTAL_THREADS`, we need to aggregate our data somehow before plotting. We can use `pandas` `pivot_table()` function to compute the `mean` of the trials for each data point.\n",
    "\n",
    "Once we have a pivot table, we can call `pandas` `plot()` to render it, then use `savefig()` to save it to the provided `filename`.\n",
    "\n",
    "Of course, you can write your own such functions, and make them arbitrarily complex/customized..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib as mpl\n",
    "\n",
    "def my_plot_func(filename, column_filters, data, series_name, x_name, y_name, exp_dict=None):\n",
    "    table = pandas.pivot_table(data, index=x_name, columns=series_name, values=y_name, aggfunc='mean')\n",
    "    table.plot(kind='line')\n",
    "    mpl.pyplot.savefig(filename)\n",
    "    print('## SAVED FIGURE {}'.format(filename))\n",
    "\n",
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools')\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 2, 4, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel 5 5 -k 200000 -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "\n",
    "    add_plot_set(\n",
    "            exp_dict\n",
    "          , name='throughput.png'\n",
    "          , title='Throughput vs data structure'\n",
    "          , series='DS_TYPENAME'\n",
    "          , x_axis='TOTAL_THREADS'\n",
    "          , y_axis='total_throughput'\n",
    "          , plot_type=my_plot_func\n",
    "    )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "disable_tee_stdout()\n",
    "run_in_jupyter(define_experiment, cmdline_args='-dp')\n",
    "enable_tee_stdout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the generated figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "display(Image('data/throughput.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing *many* plots (for data with <ins>5 dimensions</ins>)\n",
    "\n",
    "the real power of `add_plot_set` only starts to show once you want to plot *many* plots at once.\n",
    "\n",
    "so, let's add a couple of dimensions to our data:\n",
    "- key range (`MAXKEY` in the data file)\n",
    "- update rate (`INS_DEL_FRAC` in the data file)\n",
    "\n",
    "and use them to produce **multiple plots** (one for each combination of values of these dimensions). we do this by specifying `varying_cols_list` in `add_plot_set`.\n",
    "\n",
    "we can also customize the plot file`name`s and `title`s with these parameters.\n",
    "\n",
    "# Showing these plots in a table in an HTML page\n",
    "\n",
    "we also generate an HTML page to show off these grids in a table by invoking `add_page_set`.\n",
    "\n",
    "HTML page construction only occurs if you specify command line argument `-w` (which stands for `website creation`) to `run_experiment.py`. so, we add this to `run_in_jupyter`.\n",
    "\n",
    "note: you can also customize the `index.html` starting page (which is blank by default) by providing your own `HTML body` string to the function `set_content_index_html(exp_dict, content_html_string)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools') ## path to tools library\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make bin_dir={__dir_run} -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 2, 4, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "    add_run_param    (exp_dict, 'MAXKEY', [20000, 200000])\n",
    "    add_run_param    (exp_dict, 'INS_DEL_FRAC', [\"0.0 0.0\", \"5.0 5.0\"])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel {INS_DEL_FRAC} -k {MAXKEY} -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "\n",
    "    add_plot_set(\n",
    "            exp_dict\n",
    "          , name='throughput-{INS_DEL_FRAC}-{MAXKEY}k.png'\n",
    "          , title='{INS_DEL_FRAC} {MAXKEY}k: throughput'\n",
    "          , varying_cols_list=['MAXKEY', 'INS_DEL_FRAC']\n",
    "          , series='DS_TYPENAME'\n",
    "          , x_axis='TOTAL_THREADS'\n",
    "          , y_axis='total_throughput'\n",
    "          , plot_type='bars'\n",
    "    )\n",
    "\n",
    "    ## render one legend for all plots (since the legend is the same for all).\n",
    "    ## if legend varies from plot to plot, you might enable legends for all plots,\n",
    "    ## or write a custom plotting command that determines what to do, given your data\n",
    "    add_plot_set(exp_dict, name='throughput-legend.png', series='DS_TYPENAME', x_axis='TOTAL_THREADS', y_axis='total_throughput', plot_type='bars', plot_cmd_args='--legend-only --legend-columns 3')\n",
    "\n",
    "    ## we place the above legend at the bottom of *each* table by providing \"legend_file\"\n",
    "    add_page_set(\n",
    "            exp_dict\n",
    "          , image_files='throughput-{INS_DEL_FRAC}-{MAXKEY}k.png'\n",
    "          , name='throughput'\n",
    "          , column_field='INS_DEL_FRAC'\n",
    "          , row_field='MAXKEY'\n",
    "          , legend_file='throughput-legend.png'\n",
    "    )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rdpw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's view the plots produced by the previous cell\n",
    "\n",
    "note you can click on the plots to \"drill down\" into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_html('data/throughput.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about 4 dimensions?\n",
    "\n",
    "We just saw how to plot 3- and 5-dimensional data...\n",
    "\n",
    "Let's remove the `MAXKEY` column / data dimension to reduce the dimensionality of the data to 4.\n",
    "\n",
    "With only one column in the `varying_cols_list` and NO `row_field` specified in `add_page_set`, there will only be one row of plots. (So a strip of plots instead of a grid.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools') ## path to tools library\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make bin_dir={__dir_run} -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 2, 4, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "    add_run_param    (exp_dict, 'INS_DEL_FRAC', [\"0.0 0.0\", \"5.0 5.0\"])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel {INS_DEL_FRAC} -k 200000 -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "\n",
    "    add_plot_set(\n",
    "            exp_dict\n",
    "          , name='throughput-{INS_DEL_FRAC}.png'\n",
    "          , title='{INS_DEL_FRAC}: throughput'\n",
    "          , varying_cols_list=['INS_DEL_FRAC']\n",
    "          , series='DS_TYPENAME'\n",
    "          , x_axis='TOTAL_THREADS'\n",
    "          , y_axis='total_throughput'\n",
    "          , plot_type='bars'\n",
    "    )\n",
    "\n",
    "    ## render one legend for all plots (since the legend is the same for all).\n",
    "    ## if legend varies from plot to plot, you might enable legends for all plots,\n",
    "    ## or write a custom plotting command that determines what to do, given your data\n",
    "    add_plot_set(exp_dict, name='throughput-legend.png', series='DS_TYPENAME', x_axis='TOTAL_THREADS', y_axis='total_throughput', plot_type='bars', plot_cmd_args='--legend-only --legend-columns 3')\n",
    "\n",
    "    ## we place the above legend at the bottom of *each* table by providing \"legend_file\"\n",
    "    add_page_set(\n",
    "            exp_dict\n",
    "          , image_files='throughput-{INS_DEL_FRAC}.png'\n",
    "          , name='throughput'\n",
    "          , column_field='INS_DEL_FRAC'\n",
    "          , legend_file='throughput-legend.png'\n",
    "    )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rdpw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's view the plots produced by the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_html('data/throughput.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots and HTML for data with <ins>6 dimensions</ins>\n",
    "\n",
    "note that we could have added more than 2 dimensions of data (resulting in data with 6+ dimensions), listing potentially many fields in `varying_cols_list`, and this simply would have resulted in *more plots*.\n",
    "\n",
    "note that if we had **one** more dimension of data (6 dimensions in total), it could be listed in the keyword argument `table_field`, and **multiple** HTML tables would be rendered in a single HTML page (one for each value of this column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools') ## path to tools library\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make bin_dir={__dir_run} -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [2, 4, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "    add_run_param    (exp_dict, 'MAXKEY', [20000, 200000])\n",
    "    add_run_param    (exp_dict, 'INS_DEL_FRAC', ['0.0 0.0', '5.0 5.0'])\n",
    "    ## unlike the above four fields,\n",
    "    ## the run command does NOT produce a line of the form 'malloc=[...]'.\n",
    "    ## so, run_experiment.py will APPEND a line of this form to the datafile!\n",
    "    add_run_param    (exp_dict, 'malloc', ['jemalloc', 'mimalloc'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/lib{malloc}.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel {INS_DEL_FRAC} -k {MAXKEY} -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "    add_data_field   (exp_dict, 'malloc', validator=is_run_param('malloc'))\n",
    "\n",
    "    add_plot_set(\n",
    "            exp_dict\n",
    "          , name='throughput-{malloc}-{INS_DEL_FRAC}-{MAXKEY}.png'\n",
    "          , title='{malloc} {INS_DEL_FRAC} {MAXKEY}'\n",
    "          , varying_cols_list=['malloc', 'MAXKEY', 'INS_DEL_FRAC']\n",
    "          , series='DS_TYPENAME'\n",
    "          , x_axis='TOTAL_THREADS'\n",
    "          , y_axis='total_throughput'\n",
    "          , plot_type='bars'\n",
    "    )\n",
    "\n",
    "    ## render one legend for all plots (since the legend is the same for all).\n",
    "    ## if legend varies from plot to plot, you might enable legends for all plots,\n",
    "    ## or write a custom plotting command that determines what to do, given your data\n",
    "    add_plot_set(exp_dict, name='throughput-legend.png', series='DS_TYPENAME', x_axis='TOTAL_THREADS', y_axis='total_throughput', plot_type='bars', plot_cmd_args='--legend-only --legend-columns 3')\n",
    "\n",
    "    ## note: choice of column / row / table field determines how the HTML page looks -- up to you!\n",
    "    add_page_set(\n",
    "            exp_dict\n",
    "          , image_files='throughput-{malloc}-{INS_DEL_FRAC}-{MAXKEY}.png'\n",
    "          , name='throughput'\n",
    "          , column_field='INS_DEL_FRAC'\n",
    "          , row_field='MAXKEY'\n",
    "          , table_field='malloc'\n",
    "          , legend_file='throughput-legend.png'\n",
    "    )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rdpw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's view the data, plots and HTML we produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_html('data/throughput.html')\n",
    "display(select_to_dataframe('select * from data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots and HTML for data with <ins>7+ dimensions</ins>\n",
    "\n",
    "if we had MORE than one extra dimension of data (7+ dimensions in total), we could list additional fields in the keyword argument `page_field_list`, which would cause additional HTML pages to be rendered (one for each combination of values for fields in `page_field_list`), and linked together by an `index.htm`. (note that the `name` keyword argument of `page_field_list` must also be modified to reference these fields, in order for multiple HTML files to be created---you must specify what sort of naming convention you'd like the framework to use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools') ## path to tools library\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make bin_dir={__dir_run} -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [2, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "    add_run_param    (exp_dict, 'MAXKEY', [20000, 200000])\n",
    "    add_run_param    (exp_dict, 'INS_DEL_FRAC', ['0.0 0.0', '5.0 5.0'])\n",
    "    ## unlike the above four fields,\n",
    "    ## the run command does NOT produce a line of the form 'malloc=[...]'.\n",
    "    ## so, run_experiment.py will APPEND a line of this form to the datafile!\n",
    "    add_run_param    (exp_dict, 'malloc', ['jemalloc', 'mimalloc'])\n",
    "    ## ditto for reclaimer\n",
    "    add_run_param    (exp_dict, 'numactl', ['', 'numactl --interleave=all'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/lib{malloc}.so {numactl} time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel {INS_DEL_FRAC} -k {MAXKEY} -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "\n",
    "    add_plot_set(\n",
    "            exp_dict\n",
    "          , name='throughput-{malloc}-{numactl}-{INS_DEL_FRAC}-{MAXKEY}.png'\n",
    "          , title='{INS_DEL_FRAC} {MAXKEY}'\n",
    "          , varying_cols_list=['malloc', 'numactl', 'MAXKEY', 'INS_DEL_FRAC']\n",
    "          , series='DS_TYPENAME'\n",
    "          , x_axis='TOTAL_THREADS'\n",
    "          , y_axis='total_throughput'\n",
    "          , plot_type='bars'\n",
    "    )\n",
    "\n",
    "    ## render one legend for all plots (since the legend is the same for all).\n",
    "    ## if legend varies from plot to plot, you might enable legends for all plots,\n",
    "    ## or write a custom plotting command that determines what to do, given your data\n",
    "    add_plot_set(exp_dict, name='throughput-legend.png', series='DS_TYPENAME', x_axis='TOTAL_THREADS', y_axis='total_throughput', plot_type='bars', plot_cmd_args='--legend-only --legend-columns 3')\n",
    "\n",
    "    ## we place the above legend at the bottom of *each* table by providing \"legend_file\"\n",
    "    add_page_set(\n",
    "            exp_dict\n",
    "          , image_files='throughput-{malloc}-{numactl}-{INS_DEL_FRAC}-{MAXKEY}.png'\n",
    "          , name='throughput'\n",
    "          , column_field='numactl'\n",
    "          , row_field='malloc'\n",
    "          , table_field='MAXKEY'\n",
    "          , page_field_list=['INS_DEL_FRAC']\n",
    "          , legend_file='throughput-legend.png'\n",
    "    )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rdpw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's view the data, plots and HTML we produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_html('data/index.html')\n",
    "display(select_to_dataframe('select * from data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's easy to plot *many* value fields vs your `run_params`\n",
    "\n",
    "Let's go back to our 5-dimensional data example to demonstrate how to easily produce plots from *many different value fields* (not just `total_throughput`).\n",
    "\n",
    "### First let's run a quick shell command to check what kinds of fields exist in our data\n",
    "\n",
    "(This command uses `grep` with a simple `regex` to look for lines of the form \"XYZ=*number*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_to_list('grep -E \"^[^ =]+=[0-9.]+$\" data/data000001.txt', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's focus on the following fields from that list:\n",
    "\n",
    "- `tree_stats_numNodes`\n",
    "- `tree_stats_height`\n",
    "- `tree_stats_avgKeyDepth`\n",
    "- `global_epoch_counter`\n",
    "- `PAPI_L2_TCM`\n",
    "- `PAPI_L3_TCM`\n",
    "- `PAPI_TOT_CYC`\n",
    "- `PAPI_TOT_INS`\n",
    "- `total_throughput`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools')\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 2, 4, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "    add_run_param    (exp_dict, 'MAXKEY', [20000, 200000])\n",
    "    add_run_param    (exp_dict, 'INS_DEL_FRAC', [\"0.0 0.0\", \"5.0 5.0\"])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel {INS_DEL_FRAC} -k {MAXKEY} -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "    add_data_field   (exp_dict, 'tree_stats_numNodes', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'tree_stats_height', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'tree_stats_avgKeyDepth', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'global_epoch_counter', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'PAPI_L2_TCM', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_L3_TCM', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_TOT_CYC', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_TOT_INS', coltype='REAL')\n",
    "\n",
    "    ## render one legend for all plots (since the legend is the same for all).\n",
    "    ## if legend varies from plot to plot, you might enable legends for all plots,\n",
    "    ## or write a custom plotting command that determines what to do, given your data\n",
    "    add_plot_set(exp_dict, name='legend.png', series='DS_TYPENAME', x_axis='TOTAL_THREADS', y_axis='total_throughput', plot_type='bars', plot_cmd_args='--legend-only --legend-columns 3')\n",
    "\n",
    "    ## render a plot_set for EVERY numeric data field extracted above\n",
    "    for field in get_numeric_data_fields(exp_dict):\n",
    "        add_plot_set(\n",
    "              exp_dict\n",
    "            , name=field+'-{INS_DEL_FRAC}-{MAXKEY}k.png'\n",
    "            , title='{INS_DEL_FRAC} {MAXKEY}k: '+field\n",
    "            , varying_cols_list=['MAXKEY', 'INS_DEL_FRAC']\n",
    "            , series='DS_TYPENAME'\n",
    "            , x_axis='TOTAL_THREADS'\n",
    "            , y_axis=field\n",
    "            , plot_type='bars'\n",
    "        )\n",
    "\n",
    "        ## and also add a page_set for each data field.\n",
    "        ## we place the above legend at the bottom of *each* table by providing \"legend_file\"\n",
    "        add_page_set(\n",
    "              exp_dict\n",
    "            , image_files=field+'-{INS_DEL_FRAC}-{MAXKEY}k.png'\n",
    "            , name=field\n",
    "            , column_field='INS_DEL_FRAC'\n",
    "            , row_field='MAXKEY'\n",
    "            , legend_file='legend.png'\n",
    "        )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rdpw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "show_html('data/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering *many data fields* on a *single* HTML page\n",
    "\n",
    "in the previous example, we build one page for each data field extracted. however, you might want, for example, to build a single page with many data fields, each appearing as a *row* of plots.\n",
    "\n",
    "if you take a moment to think about *how* you would accomplish this using `add_page_set`, it's not obvious that you even *can*... you can specify *one field* as the `row_field`, but in this case we want to show *many different fields, one per row*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools')\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 2, 4, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "    add_run_param    (exp_dict, 'MAXKEY', [20000, 200000])\n",
    "    add_run_param    (exp_dict, 'INS_DEL_FRAC', [\"0.0 0.0\", \"5.0 5.0\"])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel {INS_DEL_FRAC} -k {MAXKEY} -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "    add_data_field   (exp_dict, 'tree_stats_numNodes', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'tree_stats_height', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'tree_stats_avgKeyDepth', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'global_epoch_counter', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'PAPI_L2_TCM', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_L3_TCM', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_TOT_CYC', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_TOT_INS', coltype='REAL')\n",
    "\n",
    "    ## render one legend for all plots\n",
    "    add_plot_set(exp_dict, name='legend.png', series='DS_TYPENAME', x_axis='TOTAL_THREADS', y_axis='total_throughput', plot_type='bars', plot_cmd_args='--legend-only --legend-columns 3')\n",
    "\n",
    "    ## render plots\n",
    "    value_fields = get_numeric_data_fields(exp_dict)\n",
    "    for field in value_fields:\n",
    "        add_plot_set(\n",
    "              exp_dict\n",
    "            , name=field+'-{INS_DEL_FRAC}-{MAXKEY}k.png'\n",
    "            , title='{INS_DEL_FRAC} {MAXKEY}k: '+field\n",
    "            , varying_cols_list=['MAXKEY', 'INS_DEL_FRAC']\n",
    "            , series='DS_TYPENAME'\n",
    "            , x_axis='TOTAL_THREADS'\n",
    "            , y_axis=field\n",
    "            , plot_type='bars'\n",
    "        )\n",
    "\n",
    "    ## and also add a page_set to show all plots\n",
    "    add_page_set(\n",
    "          exp_dict\n",
    "        , image_files='{row_field}-{INS_DEL_FRAC}-{MAXKEY}k.png'\n",
    "        , name='comparison'\n",
    "        , column_field='INS_DEL_FRAC'\n",
    "        , row_field=value_fields\n",
    "        , table_field='MAXKEY'\n",
    "        , legend_file='legend.png'\n",
    "    )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-dpw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "show_html('data/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating `tables` into different `pages`\n",
    "\n",
    "if you prefer, you can eliminate the `table_field` argument to `add_page_set` and instead use `page_field_list`. this produces a slightly different effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools')\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 2, 4, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "    add_run_param    (exp_dict, 'MAXKEY', [20000, 200000])\n",
    "    add_run_param    (exp_dict, 'INS_DEL_FRAC', [\"0.0 0.0\", \"5.0 5.0\"])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel {INS_DEL_FRAC} -k {MAXKEY} -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "    add_data_field   (exp_dict, 'tree_stats_numNodes', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'tree_stats_height', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'tree_stats_avgKeyDepth', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'global_epoch_counter', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'PAPI_L2_TCM', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_L3_TCM', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_TOT_CYC', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_TOT_INS', coltype='REAL')\n",
    "\n",
    "    ## render one legend for all plots\n",
    "    add_plot_set(exp_dict, name='legend.png', series='DS_TYPENAME', x_axis='TOTAL_THREADS', y_axis='total_throughput', plot_type='bars', plot_cmd_args='--legend-only --legend-columns 3')\n",
    "\n",
    "    ## render plots\n",
    "    value_fields = get_numeric_data_fields(exp_dict)\n",
    "    for field in value_fields:\n",
    "        add_plot_set(\n",
    "              exp_dict\n",
    "            , name=field+'-{INS_DEL_FRAC}-{MAXKEY}k.png'\n",
    "            , title='{INS_DEL_FRAC} {MAXKEY}k: '+field\n",
    "            , varying_cols_list=['MAXKEY', 'INS_DEL_FRAC']\n",
    "            , series='DS_TYPENAME'\n",
    "            , x_axis='TOTAL_THREADS'\n",
    "            , y_axis=field\n",
    "            , plot_type='bars'\n",
    "        )\n",
    "\n",
    "    ## and also add a page_set to show all plots\n",
    "    add_page_set(\n",
    "          exp_dict\n",
    "        , image_files='{row_field}-{INS_DEL_FRAC}-{MAXKEY}k.png'\n",
    "        , name='comparison'\n",
    "        , column_field='INS_DEL_FRAC'\n",
    "        , row_field=value_fields\n",
    "        , page_field_list=['MAXKEY']\n",
    "        , legend_file='legend.png'\n",
    "    )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-dpw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "show_html('data/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a `--testing` mode\n",
    "## Briefly running each configuration *BEFORE* doing a full run\n",
    "\n",
    "i often find it useful to have a `testing` mode (enabled with argument `--testing`), that runs for less time, but still explores all (important) configurations of run parameters, to make sure nothing simple will fail when i run for many hours. (fail-fast is good!)\n",
    "\n",
    "to this end, a variable called `args.testing` is accessible in `define_experiment`, and if it's `True`, then the user has passed `--testing` as a command line arg.\n",
    "\n",
    "the correct response to this is to limit the set of configurations somehow, perhaps be reducing the number of thread counts, and/or the reducing length of time to execute in each trial, and/or limiting runs to a single trial, and/or eliminating data structure prefilling (or anything else that you find appropriate).\n",
    "\n",
    "for example, let's add a simple `--testing` mode to the previous code cell.\n",
    "\n",
    "note the `if args.testing:` block, as well as the `--testing` argument passed to `run_in_jupyter` *instead of* the previous `` argument. (we also eliminate the `-r` argument, since we want to actually run our testing mode.)\n",
    "\n",
    "observe that this new `--testing` mode takes around 20 seconds to run, compared to several minutes without specifying `--testing`. (this time difference becomes much more drastic if you would normally run more trials, thread counts, or for longer than 1 second. :)) \n",
    "\n",
    "i make it a habit to run in `--testing` mode and take a quick peek at the results before running my full experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools')\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 2, 4, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "    add_run_param    (exp_dict, 'MAXKEY', [20000, 200000])\n",
    "    add_run_param    (exp_dict, 'INS_DEL_FRAC', [\"0.0 0.0\", \"5.0 5.0\"])\n",
    "\n",
    "    millis_to_run = 1000\n",
    "\n",
    "    ## defined a reduced set of configurations for testing mode\n",
    "    if args.testing:\n",
    "        add_run_param    (exp_dict, '__trials', [1])\n",
    "        add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 8])\n",
    "        millis_to_run = 100\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel {INS_DEL_FRAC} -k {MAXKEY} -t ' + str(millis_to_run))\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "    add_data_field   (exp_dict, 'tree_stats_numNodes', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'tree_stats_height', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'tree_stats_avgKeyDepth', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'global_epoch_counter', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'PAPI_L2_TCM', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_L3_TCM', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_TOT_CYC', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_TOT_INS', coltype='REAL')\n",
    "\n",
    "    ## render one legend for all plots\n",
    "    add_plot_set(exp_dict, name='legend.png', series='DS_TYPENAME', x_axis='TOTAL_THREADS', y_axis='total_throughput', plot_type='bars', plot_cmd_args='--legend-only --legend-columns 3')\n",
    "\n",
    "    ## render plots\n",
    "    value_fields = get_numeric_data_fields(exp_dict)\n",
    "    for field in value_fields:\n",
    "        add_plot_set(\n",
    "              exp_dict\n",
    "            , name=field+'-{INS_DEL_FRAC}-{MAXKEY}k.png'\n",
    "            , title='{INS_DEL_FRAC} {MAXKEY}k: '+field\n",
    "            , varying_cols_list=['MAXKEY', 'INS_DEL_FRAC']\n",
    "            , series='DS_TYPENAME'\n",
    "            , x_axis='TOTAL_THREADS'\n",
    "            , y_axis=field\n",
    "            , plot_type='bars'\n",
    "        )\n",
    "\n",
    "    ## and also add a page_set to show all plots\n",
    "    add_page_set(\n",
    "          exp_dict\n",
    "        , image_files='{row_field}-{INS_DEL_FRAC}-{MAXKEY}k.png'\n",
    "        , name='comparison'\n",
    "        , column_field='INS_DEL_FRAC'\n",
    "        , row_field=value_fields\n",
    "        , page_field_list=['MAXKEY']\n",
    "        , legend_file='legend.png'\n",
    "    )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='--testing -rdpw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the `--testing` mode results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "show_html('data/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom output filename patterns\n",
    "\n",
    "in the experiments above, we have always used the default filename for output files: `dataXXXXXX.txt`.\n",
    "\n",
    "if you want a different file naming scheme, it's easy to specify a pattern for this using `set_file_data(exp_dict, pattern)`.\n",
    "\n",
    "let's see an example of this, where we include the current values of several `run_param`s in the outfile file pattern.\n",
    "\n",
    "(you can also set the output directory with `set_dir_data(exp_dict, path)`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools')\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 2, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "    add_run_param    (exp_dict, 'MAXKEY', [20000, 200000])\n",
    "    add_run_param    (exp_dict, 'INS_DEL_FRAC', [\"0.0 0.0\", \"5.0 5.0\"])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel {INS_DEL_FRAC} -k {MAXKEY} -t 1000')\n",
    "\n",
    "    set_file_data    (exp_dict, 'my_data_n{TOTAL_THREADS}_k{MAXKEY}_insdel{INS_DEL_FRAC}_{DS_TYPENAME}.txt')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='--testing -rdpw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic best-effort sanity checks\n",
    "\n",
    "the data framework does its best to identify some basic mistakes that are common when running repeated experiments over a large configuration space. we describe some of them here, and show how they work.\n",
    "\n",
    "for example, observe that the following `define_experiment` function attempts to plot `TOTAL_THREADS` on the x-axis, `total_throughput` on the y-axis, with `DS_TYPENAME` as the series, but completely ignores `MAXKEY` in the `add_plot_set` call.\n",
    "\n",
    "this is a mistake, as this would result in `averaging` unrelated data points with two *different* values of `MAXKEY`.\n",
    "\n",
    "run the following code cell to see the detailed error message that results in this situation. it attempts to be as helpful as possible in helping you diagnose the cause. in this case it essentially identifies and highlights the problematic column (`MAXKEY`) *for you*, and suggests a fix (adding it to the `varying_cols_list` argument when calling `add_plot_set`).\n",
    "\n",
    "of course, just because something plots successfully doesn't mean you haven't made a mistake... but we do our best to catch a variety of simple mistakes. (or at least assert and fail-fast when *some* sensible assumptions are violated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools')\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "    add_run_param    (exp_dict, 'MAXKEY', [20000, 200000])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel 0.5 0.5 -k {MAXKEY} -t 100')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "\n",
    "    add_plot_set(\n",
    "          exp_dict\n",
    "        , name='throughput.png'\n",
    "        , title='throughput'\n",
    "        , series='DS_TYPENAME'\n",
    "        , x_axis='TOTAL_THREADS'\n",
    "        , y_axis='total_throughput'\n",
    "        , plot_type='bars'\n",
    "    )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rdp', error_exit_code=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic archival features\n",
    "## (data zip, git commit hash fetch, git diff file archive)\n",
    "\n",
    "activated with command line arg: `-z` (which stands for `zip creation`)\n",
    "\n",
    "the data framework offers a powerful convenience for archiving your experiments: it can automatically ZIP *as little data as is needed* to guarantee you won't lose the ability to return to this exact code/data state (file/directory structure).\n",
    "\n",
    "how does it do this?\n",
    "\n",
    "well, assuming you are working a git repository, and are committing changes as you go, the repository's current `commit hash` presumably gives you a way to get *pretty close* to your current file/directory structure.\n",
    "\n",
    "but of course, it will be missing any changes you've made since your last commit! this includes all of the data you've just generated, as well as any tentative code changes you've made (perhaps experimental changes you're currently testing).\n",
    "\n",
    "happily, we can *extract* the list of files you've changed *since your last commit* directly with a `git` command: `git status -s | awk '{if ($1 != \"D\") print $2}' | grep -v \"/$\"`\n",
    "\n",
    "so, we do this, and then once we have this list of files, we selectively add *them* to a ZIP file along with the data directory we just produced, as well as the file `output_log.txt`.\n",
    "\n",
    "crucially, any files that are ignored by `git` (because they are covered by a pattern in your `.gitignore` file) will *NOT* be added to the ZIP file. this means you can automatically exclude files easily that you wouldn't want in your repo anyway. (normally the `data` folder produced by your experiments would probably fall into that category, but we add it manually. if you want to add more files manually, see the function `do_finish` in `run_experiment.py`.)\n",
    "\n",
    "this whole process should make it easier to achieve a *much* smaller file size for an archive that you *can* reconstruct to reproduce experiments. this smaller file size *should* make it feasible to archive *every* set of experiments you run by default, along with enough information to understand exactly what was run, later. (and, you should only occasionally have to clean up your archives.) \n",
    "\n",
    "this can help you eliminate one of the questions we all *hate* asking: `what on earth did we run to get these results?`\n",
    "\n",
    "to help you reconstruct your current file/directory state later, we dump all relevant information about the `current commit`, including the `commit hash` to `output_log.txt` before we add it to the ZIP. you can find this information about the commit by looking for `'git status:'` or `'commit hash='` in `output_log.txt`.\n",
    "\n",
    "for example, the following code causes text along the following lines to be archived as part of `output_log.txt`:\n",
    "\n",
    "    ## ## Fetching git status and any uncommitted changes for archival purposes\n",
    "    ## \n",
    "    ## commit_hash=05ec0e2184bd8c7a30e22457483cbeeadd0c2461\n",
    "    ## git_status:\n",
    "    ## On branch data_framework\n",
    "    ## Your branch is up to date with 'origin/data_framework'.\n",
    "    ## \n",
    "    ## Changes not staged for commit:\n",
    "    ##   (use \"git add <file>...\" to update what will be committed)\n",
    "    ##   (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
    "    ##   (commit or discard the untracked or modified content in submodules)\n",
    "    ## \n",
    "    ## \tmodified:   .vscode/settings.json\n",
    "    ## \tmodified:   microbench_experiments/tutorial/tutorial.ipynb\n",
    "    ## \tmodified:   microbench_experiments/tutorial/tutorial_extra.ipynb\n",
    "    ## \tmodified:   tools (new commits, modified content)\n",
    "    ## \n",
    "    ## no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
    "    ## \n",
    "    ## diff_files=['.vscode/settings.json', 'microbench_experiments/tutorial/tutorial.ipynb', 'microbench_experiments/tutorial/tutorial_extra.ipynb', 'tools']\n",
    "\n",
    "## on my system, the following code produces an archive smaller than `3MB`, which offers complete reproducibility (and even includes 37 generated plots), despite the entire contents of setbench reaching `140MB`!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools    (exp_dict, os.getcwd() + '/../../tools')\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j6')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2])\n",
    "    add_run_param    (exp_dict, 'TOTAL_THREADS', [1, 2, 4, 8])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "    add_run_param    (exp_dict, 'MAXKEY', [20000, 200000])\n",
    "    add_run_param    (exp_dict, 'INS_DEL_FRAC', [\"0.0 0.0\", \"5.0 5.0\"])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./{DS_TYPENAME}.debra -nwork {TOTAL_THREADS} -nprefill {TOTAL_THREADS} -insdel {INS_DEL_FRAC} -k {MAXKEY} -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'total_throughput', coltype='INTEGER', validator=is_positive)\n",
    "    add_data_field   (exp_dict, 'tree_stats_numNodes', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'tree_stats_height', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'tree_stats_avgKeyDepth', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'global_epoch_counter', coltype='INTEGER')\n",
    "    add_data_field   (exp_dict, 'PAPI_L2_TCM', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_L3_TCM', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_TOT_CYC', coltype='REAL')\n",
    "    add_data_field   (exp_dict, 'PAPI_TOT_INS', coltype='REAL')\n",
    "\n",
    "    ## render one legend for all plots\n",
    "    add_plot_set(exp_dict, name='legend.png', series='DS_TYPENAME', x_axis='TOTAL_THREADS', y_axis='total_throughput', plot_type='bars', plot_cmd_args='--legend-only --legend-columns 3')\n",
    "\n",
    "    ## render plots\n",
    "    value_fields = get_numeric_data_fields(exp_dict)\n",
    "    for field in value_fields:\n",
    "        add_plot_set(\n",
    "              exp_dict\n",
    "            , name=field+'-{INS_DEL_FRAC}-{MAXKEY}k.png'\n",
    "            , title='{INS_DEL_FRAC} {MAXKEY}k: '+field\n",
    "            , varying_cols_list=['MAXKEY', 'INS_DEL_FRAC']\n",
    "            , series='DS_TYPENAME'\n",
    "            , x_axis='TOTAL_THREADS'\n",
    "            , y_axis=field\n",
    "            , plot_type='bars'\n",
    "        )\n",
    "\n",
    "    ## and also add a page_set to show all plots\n",
    "    add_page_set(\n",
    "          exp_dict\n",
    "        , image_files='{row_field}-{INS_DEL_FRAC}-{MAXKEY}k.png'\n",
    "        , name='comparison'\n",
    "        , column_field='INS_DEL_FRAC'\n",
    "        , row_field=value_fields\n",
    "        , table_field='MAXKEY'\n",
    "        , legend_file='legend.png'\n",
    "    )\n",
    "\n",
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "run_in_jupyter(define_experiment, cmdline_args='-rdpwz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595426832078",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}